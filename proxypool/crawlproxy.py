#获取ip,主要是爬取ip,将获取到的ip存储到数据库,根据需求量，定时爬取
import requests
import manageip
import threading
import ckeckip
import json

class CrawlProxy(object):

    def __init__(self,req_url,max_size=100,time=30):
        #设置数据库中存储的代理的最大的量，超过200,则暂停爬取
        self.max_size = max_size
        self.headers = {
            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:60.0) Gecko/20100101 Firefox/60.0',
        }
        self.url = req_url
        self.time = time
        self.managerIp = manageip.ManageIp(database='proxydb',col_name='proxycol')
        self.checkIp = ckeckip.CheckIp(test_url='https://www.qichacha.com/')
        self.thread = threading.Timer(self.time, self.check_need_get_proxy)
        self.thread.start()

    #定时比对如果代理池中的代理数小于100,则发起请求获取代理
    def check_need_get_proxy(self):
        print('－－－－－－正在对比代理的数量－－－－－－')
        if self.managerIp.get_size() < self.max_size:
            self.crawl_proxy_data()

    def crawl_proxy_data(self):
        print('正在下载')
        if self.managerIp.get_size() < self.max_size:
            print('目前总数量小于最大代理存储量,正在获取新的代理')
            #发起请求，获取响应结果
            response = requests.get(self.url,headers=self.headers)
            if response.status_code == 200:
                # 请求成功,则将获取的数据写入redis数据库
                html = response.text
                # json_data = json.loads(html)
                proxy_list = [i.replace('\r','') for i in html.split('\n')]

                # proxy_list = ['42.48.118.106:37178', '222.128.9.235:33428', '59.172.27.6:38380', '124.42.68.152:90', '183.196.97.125:41397', '27.22.104.28:30533', '183.129.207.84:57468', '183.129.207.84:49662', '119.145.136.126:8888', '139.159.7.150:52908', '139.196.51.201:8118', '58.240.7.195:38618', '183.129.207.84:26550', '117.131.99.210:53281', '117.159.23.115:38120', '111.230.221.205:80', '101.251.255.50:40871', '116.224.105.132:8888', '112.95.26.67:8118', '221.6.32.206:41816', '183.129.207.82:11415', '221.239.86.26:32228', '118.182.33.6:42801', '139.199.201.82:8118', '183.129.207.84:11623', '42.51.3.89:8080', '58.210.133.98:58677', '117.21.191.151:49862', '139.199.117.41:8118', '120.77.247.147:80', '218.7.221.166:50159', '36.7.128.146:44473', '183.129.207.84:13245', '119.123.176.61:8888', '123.235.32.36:45611', '111.8.150.102:44680', '120.77.205.21:80', '47.97.112.249:80', '60.216.101.46:32868', '112.115.163.76:53281', '58.251.49.4:58729', '210.72.14.142:80', '202.103.12.30:50356', '182.92.113.183:8118', '47.106.92.90:8081', '183.129.207.84:20658', '218.66.79.175:52077', '14.23.114.92:81', '140.143.170.222:8118', '124.235.181.175:80', '223.203.0.14:8000', '112.74.187.216:80', '120.79.174.103:8118', '183.129.244.17:20195', '183.129.207.84:10988', '182.207.232.135:49166', '163.125.235.215:8118', '121.196.196.105:80', '183.129.207.84:57319', '211.101.136.86:49784', '118.178.170.146:80', '163.125.235.202:8118', '182.92.113.148:8118', '202.199.159.130:40670', '119.254.94.114:34422', '221.224.136.211:35101', '123.103.93.38:80', '117.34.65.147:1080', '58.210.136.83:30498', '59.57.151.126:31689', '61.184.109.33:50371', '183.129.207.84:11187', '163.125.28.70:8118', '119.254.94.97:8888', '124.235.135.166:80', '58.56.108.226:58690', '113.107.173.92:44045', '112.74.33.126:80', '116.228.53.234:43792', '193.112.15.70:8118', '47.95.157.182:3200', '106.15.42.179:33543', '223.100.166.3:36945', '183.129.207.84:11470', '119.254.94.95:56942', '183.129.207.84:56940', '60.173.244.133:35634', '218.204.129.195:43809', '183.129.207.82:12134', '218.90.174.37:34749', '211.136.127.125:80', '218.86.87.171:31661', '203.93.125.238:31566', '183.129.207.80:41766', '36.110.234.244:80', '175.155.75.242:8888', '183.45.88.109:32925', '61.176.223.7:58822', '115.29.170.58:8118', '124.235.145.79:80']

                if len(proxy_list) > 0:
                    print('已获取取:',len(proxy_list),'条')
                    #将获取的所有HTTPS协议类型的并且是高匿ip，检测ip将可用ip，存放进数据库
                    self.checkIp.check_proxy_with_proxies(proxies=proxy_list)
                    #如果不需要做入库前的检测,就先将代理放入数据库,后面做同意的检测
                    #self.managerIp.save_proxies(proxy_list)












